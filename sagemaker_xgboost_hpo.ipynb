{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker with XGBoost and Hyperparameter Tuning for Direct Marketing predictions \n",
    "_**Supervised Learning with Gradient Boosted Trees: A Binary Prediction Problem With Unbalanced Classes**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background)\n",
    "1. [Environment Prepration](#Environment-preparation)\n",
    "1. [Data Downloading](#Data-downloading-and-exploration)\n",
    "1. [Data Transformation](#Data-Transformation)\n",
    "1. [SageMaker: Training](#Training)\n",
    "1. [SageMaker: Deploying and evaluating model](#Deploying-and-evaluating-model)\n",
    "1. [SageMaker: Hyperparameter Optimization (HPO)](#Hyperparameter-Optimization-(HPO))\n",
    "1. [Conclusions](#Conclusions)\n",
    "1. [Releasing cloud resources](#Releasing-cloud-resources)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "The goal of this workshop is to serve as a **Minimum Viable Example about SageMaker**, teaching you how to do a **basic ML training** and **Hyper-Parameter Optimization (HPO)** in AWS. Teaching an in-depth Data Science approach is out of the scope of this workshop. We hope that you can use it as a starting point and modify it according to your future projects. \n",
    "\n",
    "---\n",
    "\n",
    "## Background (problem description and approach)\n",
    "\n",
    "- **Direct marketing**: contacting potential new customers via mail, email, phone call etc. \n",
    "- **Challenge**: A) too many potential customers. B) limited resources of the approacher (time, money etc.).\n",
    "- **Problem: Which are the potential customers with the higher chance of becoming actual customers**? (so as to focus the effort only on them). \n",
    "- **Our setting**: A bank who wants to predict *whether a customer will enroll for a term deposit, after one or more phone calls*.\n",
    "- **Our approach**: Build a ML model to do this prediction, from readily available information e.g. demographics, past interactions etc. (features).\n",
    "- **Our tools**: We will be using the **XGBoost** algorithm in AWS **SageMaker**, followed by **Hyperparameter Optimization (HPO)** to produce the best model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Environment preparation\n",
    "\n",
    "SageMaker requires some minimal setup at the begining. This setup is standard and you can use it for any of your future projects.  \n",
    "Things to specify:\n",
    "- The **S3 bucket** and **prefix** that you want to use for training and model data. **This should be within the same region as SageMaker training**!\n",
    "- The **IAM role** used to give training access to your data. See SageMaker documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # For matrix operations and numerical processing\n",
    "import pandas as pd  # For munging tabular data\n",
    "import time\n",
    "import os\n",
    "from util.ml_reporting_tools import generate_classification_report  # helper function for classification reports\n",
    "\n",
    "# setting up SageMaker parameters\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sgmk_region = boto3.Session().region_name    \n",
    "sgmk_client = boto3.Session().client(\"sagemaker\")\n",
    "sgmk_role = sagemaker.get_execution_role()\n",
    "sgmk_bucket = sagemaker.Session().default_bucket()  # a default bucket has been created for this session\n",
    "sgmk_prefix = \"sagemaker/xgboost-hpo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data downloading and exploration\n",
    "Let's start by downloading the [direct marketing dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) from UCI's ML Repository.  \n",
    "We can run shell commands from Jupyter using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Running shell commands from Jupyter)\n",
    "!wget -P data/ -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
    "!unzip -o data/bank-additional.zip -d data/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets read this into a Pandas data frame and take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"./data/bank-additional/bank-additional-full.csv\", sep=\";\")\n",
    "df_data.head()  # show part of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Specifics on each of the features:**_\n",
    "\n",
    "*Demographics:*\n",
    "* `age`: Customer's age (numeric)\n",
    "* `job`: Type of job (categorical: 'admin.', 'services', ...)\n",
    "* `marital`: Marital status (categorical: 'married', 'single', ...)\n",
    "* `education`: Level of education (categorical: 'basic.4y', 'high.school', ...)\n",
    "\n",
    "*Past customer events:*\n",
    "* `default`: Has credit in default? (categorical: 'no', 'unknown', ...)\n",
    "* `housing`: Has housing loan? (categorical: 'no', 'yes', ...)\n",
    "* `loan`: Has personal loan? (categorical: 'no', 'yes', ...)\n",
    "\n",
    "*Past direct marketing contacts:*\n",
    "* `contact`: Contact communication type (categorical: 'cellular', 'telephone', ...)\n",
    "* `month`: Last contact month of year (categorical: 'may', 'nov', ...)\n",
    "* `day_of_week`: Last contact day of the week (categorical: 'mon', 'fri', ...)\n",
    "* `duration`: Last contact duration, in seconds (numeric). Important note: If duration = 0 then `y` = 'no'.\n",
    " \n",
    "*Campaign information:*\n",
    "* `campaign`: Number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "* `pdays`: Number of days that passed by after the client was last contacted from a previous campaign (numeric)\n",
    "* `previous`: Number of contacts performed before this campaign and for this client (numeric)\n",
    "* `poutcome`: Outcome of the previous marketing campaign (categorical: 'nonexistent','success', ...)\n",
    "\n",
    "*External environment factors:*\n",
    "* `emp.var.rate`: Employment variation rate - quarterly indicator (numeric)\n",
    "* `cons.price.idx`: Consumer price index - monthly indicator (numeric)\n",
    "* `cons.conf.idx`: Consumer confidence index - monthly indicator (numeric)\n",
    "* `euribor3m`: Euribor 3 month rate - daily indicator (numeric)\n",
    "* `nr.employed`: Number of employees - quarterly indicator (numeric)\n",
    "\n",
    "*Target variable* **(the one we want to eventually predict):**\n",
    "* `y`: Has the client subscribed to a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Transformation\n",
    "Cleaning up data is part of nearly every ML project. Several common steps include:\n",
    "\n",
    "* **Handling missing values**: In our case there are no missing values.\n",
    "* **Handling weird/outlier values**: There are some values in the dataset that may require manipulation.\n",
    "* **Converting categorical to numeric**: There are a lot of categorical variables in our dataset. We need to address this.\n",
    "* **Oddly distributed data**: We will be using XGBoost, which is a non-linear method, and is minimally affected by the data distribution.\n",
    "* **Remove unnecessary data**: There are lots of columns representing general economic features that may not be available during inference time.\n",
    "\n",
    "To summarise, we need to A) address some weird values, B) convert the categorical to numeric valriables and C) Remove unnecessary data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Many records have the value of \"999\" for `pdays`. It is very likely to be a 'magic' number to represent that *no contact was made before*. Considering that, we will create a new column called \"no_previous_contact\", then grant it value of \"1\" when pdays is 999 and \"0\" otherwise.\n",
    "\n",
    "2. In the `job` column, there are more than one categories for people who don't work e.g., \"student\", \"retired\", and \"unemployed\". It is very likely the decision to enroll or not to a term deposit depends a lot on whether the customer is working or not. A such, we generate a new column to show whether the customer is working based on `job` column.\n",
    "\n",
    "3. We will remove the economic features and `duration` from our data as they would need to be forecasted with high precision to be used as features during inference time.\n",
    "\n",
    "4. We convert categorical variables to numeric using *one hot encoding*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator variable to capture when pdays takes a value of 999\n",
    "df_data[\"no_previous_contact\"] = np.where(df_data[\"pdays\"] == 999, 1, 0)\n",
    "\n",
    "# Indicator for individuals not actively employed\n",
    "df_data[\"not_working\"] = np.where(np.in1d(df_data[\"job\"], [\"student\", \"retired\", \"unemployed\"]), 1, 0)\n",
    "\n",
    "# remove unnecessary data\n",
    "df_model_data = df_data.drop(\n",
    "    [\"duration\", \n",
    "    \"emp.var.rate\", \n",
    "    \"cons.price.idx\", \n",
    "    \"cons.conf.idx\", \n",
    "    \"euribor3m\", \n",
    "    \"nr.employed\"], \n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df_model_data = pd.get_dummies(df_model_data)  # Convert categorical variables to sets of indicators\n",
    "\n",
    "df_model_data.head()  # Show part of the new transformed dataframe (which will be used for training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training\n",
    "\n",
    "Before initializing training, there are some things that need to be done:\n",
    "1. Suffle and split dataset. \n",
    "2. Convert the dataset to the right format the SageMaker algorithm expects (e.g. CSV).\n",
    "3. Copy the dataset to S3 in order to be accessed by SageMaker during training. \n",
    "4. Create s3_inputs that our training function can use as a pointer to the files in S3.\n",
    "5. Specify the ECR container location for SageMaker's implementation of XGBoost.\n",
    "\n",
    "We will shuffle and split the dataset into **Training (70%)**, **Validation (20%)**, and **Test (10%)**. We will use the Training and Validation splits during the training phase, while the 'holdout' Test split will be used to evaluate the model performance after it is deployed to production.  \n",
    "\n",
    "Amazon SageMaker's XGBoost algorithm expects data in the **libSVM** or **CSV** formats. For the CSV format, the following specifications should be met:\n",
    "- The first column must be the target variable.\n",
    "- No headers should be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and splitting dataset\n",
    "train_data, validation_data, test_data = np.split(\n",
    "    df_model_data.sample(frac=1, random_state=1729), \n",
    "    [int(0.7 * len(df_model_data)), int(0.9*len(df_model_data))],\n",
    ") \n",
    "\n",
    "# create CSV files for Train / Validation / Test\n",
    "# XGBoost expects a CSV file with no headers, with the 1st row being the ground truth\n",
    "# We are preparing such a CSV file in the following lines\n",
    "pd.concat([train_data[\"y_yes\"], train_data.drop([\"y_no\", \"y_yes\"], axis=1)], axis=1).to_csv(\"data/train.csv\", index=False, header=False)\n",
    "pd.concat([validation_data[\"y_yes\"], validation_data.drop([\"y_no\", \"y_yes\"], axis=1)], axis=1).to_csv(\"data/validation.csv\", index=False, header=False)\n",
    "pd.concat([test_data[\"y_yes\"], test_data.drop([\"y_no\", \"y_yes\"], axis=1)], axis=1).to_csv(\"data/test.csv\", index=False, header=False)\n",
    "\n",
    "# copy CSV files to S3 for SageMaker training (training files should reside in S3)\n",
    "boto3.Session().resource(\"s3\").Bucket(sgmk_bucket).Object(os.path.join(sgmk_prefix, \"train.csv\")).upload_file(\"data/train.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(sgmk_bucket).Object(os.path.join(sgmk_prefix, \"validation.csv\")).upload_file(\"data/validation.csv\")\n",
    "\n",
    "# create s3_inputs channels (objects pointing to the S3 locations)\n",
    "s3_input_train = sagemaker.s3_input(s3_data=\"s3://{}/{}/train\".format(sgmk_bucket, sgmk_prefix), content_type=\"csv\")\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=\"s3://{}/{}/validation\".format(sgmk_bucket, sgmk_prefix), content_type=\"csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify algorithm container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify object of the xgboost container image\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "xgb_container_image = get_image_uri(sgmk_region, \"xgboost\", repo_version=\"latest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small competition: try to predict the best values for 4 hyper-parameters!\n",
    "SageMaker's XGBoost includes 38 parameters. You can find more information about them [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html).\n",
    "For simplicity, we choose to experiment only with 6 of them.\n",
    "\n",
    "**Please select values for the 4 hyperparameters (by replacing the \"?\") based on the provided ranges.** Later we will see which model performed best and compare it with the one from the Hyperparameter Optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()  # initiate a SageMaker session\n",
    "\n",
    "# instantiate an XGBoost estimator object\n",
    "xgb_estimator = sagemaker.estimator.Estimator(\n",
    "    image_name=xgb_container_image,           # XGBoost algorithm container\n",
    "    role=sgmk_role,                      # IAM role to be used\n",
    "    train_instance_type=\"ml.m4.xlarge\",  # type of training instance\n",
    "    train_instance_count=1,              # number of instances to be used\n",
    "    output_path=\"s3://{}/{}/output\".format(sgmk_bucket, sgmk_prefix),\n",
    "    sagemaker_session=sess,\n",
    "    train_use_spot_instances=True,       # Use spot instances to reduce cost\n",
    "    train_max_run=20*60,                 # Maximum allowed active runtime\n",
    "    train_max_wait=30*60,                # Maximum clock time (including spot delays)\n",
    ")\n",
    "\n",
    "# scale_pos_weight is a paramater that controls the relative weights of the classes.\n",
    "# Because the data set is so highly skewed, we set this parameter according to the ratio (y_no/y_yes)\n",
    "scale_pos_weight = np.count_nonzero(train_data[\"y_yes\"].values==0) / np.count_nonzero(train_data[\"y_yes\"].values)\n",
    "\n",
    "# define its hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    num_round=?,     # int: [1,300]\n",
    "    max_depth=?,     # int: [1,10]\n",
    "    alpha=?,         # float: [0,5]\n",
    "    eta=?,           # float: [0,1]\n",
    "    silent=0,\n",
    "    objective=\"binary:logistic\",\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "\n",
    "xgb_estimator.fit({\"train\": s3_input_train, \"validation\": s3_input_validation}, wait=True)  # start a training (fitting) job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deploying and evaluating model\n",
    "\n",
    "### Deployment\n",
    "Now that we've trained the xgboost algorithm on our data, deploying the model (hosting it behind a real-time endpoint) is just one line of code!\n",
    "\n",
    "*Attention! This may take up to 10 minutes, depending on the AWS instance you select*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "First we'll need to determine how we pass data into and receive data from our endpoint. Our data is currently stored as NumPy a array in memory of our notebook instance. To send it in an HTTP POST request, we will serialize it as a CSV string and then decode the resulting CSV.  \n",
    "Note: For inference with CSV format, SageMaker XGBoost requires that the data **does NOT include the target variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting strings for HTTP POST requests on inference\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "def predict_prob(predictor, data):\n",
    "    # predictor settings\n",
    "    predictor.content_type = \"text/csv\"\n",
    "    predictor.serializer = csv_serializer\n",
    "    return np.fromstring(predictor.predict(data).decode(\"utf-8\"), sep=\",\")  # convert back to numpy \n",
    "\n",
    "\n",
    "# getting the predicted probabilities \n",
    "predictions = predict_prob(xgb_predictor, test_data.drop([\"y_no\", \"y_yes\"], axis=1).values)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are the **predicted probabilities** (in the interval [0,1]) of a potential customer enrolling for a term deposit. \n",
    "- 0: the person WILL NOT enroll.\n",
    "- 1: the person WILL enroll (which makes him/her good candidate for direct marketing).\n",
    "\n",
    "Now we will generate a **comprehensive model report**, using the following functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_classification_report(\n",
    "    y_actual=test_data[\"y_yes\"].values, \n",
    "    y_predict_proba=predictions, \n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"Did not enroll\",\"Enrolled\"],\n",
    "    model_info=\"XGBoost SageMaker inbuilt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameter Optimization (HPO)\n",
    "*Note, with the default setting below, the hyperparameter tuning job can take up to 30 minutes to complete.*\n",
    "\n",
    "We will use SageMaker HyperParameter Optimization (HPO) to automate the searching process effectively. Specifically, we **specify a range**, or a list of possible values in the case of categorical hyperparameters, for each of the hyperparameter that we plan to tune.  \n",
    "\n",
    "We will tune 4 hyperparameters in this example:\n",
    "* **eta**: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative. \n",
    "* **alpha**: L1 regularization term on weights. Increasing this value makes models more conservative. \n",
    "* **min_child_weight**: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is. \n",
    "* **max_depth**: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted. \n",
    "\n",
    "SageMaker hyperparameter tuning will automatically launch **multiple training jobs** with different hyperparameter settings, evaluate results of those training jobs based on a predefined \"objective metric\", and select the hyperparameter settings for future attempts based on previous results. For each hyperparameter tuning job, we will specify the maximum number of HPO tries (`max_jobs`) and how many of these can happen in parallel (`max_parallel_jobs`).\n",
    "\n",
    "Tip: `max_parallel_jobs` creates a **trade-off between parformance and speed** (better hyperparameter values vs how long it takes to find these values). If `max_parallel_jobs` is large, then HPO is faster, but the discovered values may not be optimal. Smaller `max_parallel_jobs` will increase the chance of finding optimal values, but HPO will take more time to finish.\n",
    "\n",
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: **validation:auc** and **train:auc**, and we elected to monitor *validation:auc* as you can see below. In this case, we only need to specify the metric name and do not need to provide regex.  \n",
    "\n",
    "If you bring your own algorithm, your algorithm emits metrics by itself. In that case, you'll need to add a MetricDefinition object here to define the format of those metrics through regex, so that SageMaker knows how to extract those metrics from your CloudWatch logs.\n",
    "\n",
    "For more information on the documentation of the Sagemaker HPO please refer [here](https://sagemaker.readthedocs.io/en/stable/tuner.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required HPO objects\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# set up hyperparameter ranges\n",
    "ranges = {\n",
    "    \"num_round\": IntegerParameter(1, 300),\n",
    "    \"max_depth\": IntegerParameter(1, 10),\n",
    "    \"alpha\": ContinuousParameter(0, 5),\n",
    "    \"eta\": ContinuousParameter(0, 1)\n",
    "}\n",
    "\n",
    "# set up the objective metric\n",
    "objective = \"validation:auc\"\n",
    "\n",
    "# instantiate a HPO object\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=xgb_estimator,          # the SageMaker estimator object\n",
    "    objective_metric_name=objective,  # the objective metric to be used for HPO\n",
    "    hyperparameter_ranges=ranges,     # the range of hyperparameters\n",
    "    max_jobs=20,                      # total number of HPO jobs\n",
    "    max_parallel_jobs=4,              # how many HPO jobs can run in parallel\n",
    "    strategy=\"Bayesian\",              # the internal optimization strategy of HPO\n",
    "    objective_type=\"Maximize\"         # maximize or minimize the objective metric\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch HPO\n",
    "Now we can launch a hyperparameter tuning job by calling *fit()* function. After the hyperparameter tuning job is created, we can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start HPO\n",
    "tuner.fit({\"train\": s3_input_train, \"validation\": s3_input_validation}, include_cls_metadata=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important notice**: HPO jobs are expected to take quite long to finsih and as such, **they do not wait by default** (the cell will look as 'done' while the job will still be running on the cloud). As such, all subsequent cells relying on the HPO output cannot run unless the job is finished. In order to check whether the HPO has finished (so we can proceed with executing the subsequent code) we can run the following polling script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait, until HPO is finished\n",
    "hpo_state = \"InProgress\"\n",
    "\n",
    "while hpo_state == \"InProgress\":\n",
    "    hpo_state = sgmk_client.describe_hyper_parameter_tuning_job(\n",
    "                HyperParameterTuningJobName=tuner.latest_tuning_job.job_name)[\"HyperParameterTuningJobStatus\"]\n",
    "    print(\"-\", end=\"\")\n",
    "    time.sleep(60)  # poll once every 1 min\n",
    "\n",
    "print(\"\\nHPO state:\", hpo_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy and test optimized model\n",
    "Deploying the best model is simply one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy the best model from HPO\n",
    "best_model_predictor = tuner.deploy(initial_instance_count=1, instance_type=\"ml.m5.large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once deployed, we can now evaluate the performance of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predicted probabilities of the best model\n",
    "predictions = predict_prob(best_model_predictor, test_data.drop([\"y_no\", \"y_yes\"], axis=1).values)\n",
    "print(predictions)\n",
    "\n",
    "# generate report for the best model\n",
    "generate_classification_report(\n",
    "    y_actual=test_data[\"y_yes\"].values, \n",
    "    y_predict_proba=predictions, \n",
    "    decision_threshold=0.5,\n",
    "    class_names_list=[\"Did not enroll\",\"Enrolled\"],\n",
    "    model_info=\"XGBoost SageMaker inbuilt + HPO\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "The optimized HPO model exhibits approximately AUC=0.773.\n",
    "Depending on the number of tries, HPO can give a better performing model, compared to simply trying different hyperparameters (by trial and error).  \n",
    "You can learn more in-depth details about HPO [here](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Releasing cloud resources\n",
    "\n",
    "It is generally a good practice to deactivate all endpoints which are not in use.  \n",
    "Please uncomment the following lines and run the cell in order to deactive the 2 endpoints that were created before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "# best_model_predictor.delete_endpoint(delete_endpoint_config=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
